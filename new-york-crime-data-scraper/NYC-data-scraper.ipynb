{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lots of imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from dateutil.parser import parse\n",
    "import warnings, os, time, requests\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New York Crime Data Scraper.\n",
    "* <b>Data Sources</b> -\n",
    "    * The data for this analysis is taken from two sources namely - \n",
    "        - [NYPD Complaint Data Historic](https://data.cityofnewyork.us/Public-Safety/NYPD-Complaint-Data-Historic/qgea-i56i)\n",
    "           : This dataset includes all valid felony, misdemeanor, and violation crimes reported to the New York City Police Department (NYPD) from 2006 to the end of 2017 \n",
    "        - [NYPD Complaint Data Year To Date](https://data.cityofnewyork.us/Public-Safety/NYPD-Complaint-Data-Current-Year-To-Date-/5uac-w243)\n",
    "           : This dataset includes all valid felony, misdemeanor, and violation crimes reported to the New York City Police Department (NYPD) for all complete quarters so far from 2018 to 2019\n",
    "        \n",
    "    * Both the datasets belong to the New York Crime Department. This dataset is publicly avaiable via the Socrata Open Data API. More about the Socrata Open Data API can be found [here](https://dev.socrata.com/)\n",
    "    \n",
    "\n",
    "* <b>Note on limitations</b> - \n",
    "    1. Historic data has been collected from 2013 to 2017. For each year 6 months data has been collected from January to June\n",
    "    2. Year to Date data has been collected from 2018 to 2019. For each year 6 months data has been collected from January to June\n",
    "    3. Manually change dates in the scraping utility to get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set default configuration variables that can be used globally\n",
    "\n",
    "1. `setDefaults()` : acts as setter function that initialises global variables which can be referenced throughout the notebook \n",
    "2. `isValid()` : validation checks to check if the request parameters and other globals are passed correctly to the API  \n",
    "3. `isValidAndSetDefaults()` : helper function for setDefaults() and isValid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_ = {key : None for key in ['APP_TOKEN', 'LIMIT', 'DATASET', 'BASE_URL', 'URI_IDENTIFIERS', 'SAVE_TO_DIRECTORY', \\\n",
    "                                                                 'START_DATE', 'END_DATE', 'RES_PER_RESPONSE']}\n",
    "\n",
    "def setDefaults(dataset, start_date, end_date, save_to_directory, results_per_response=10000, limit=100000):\n",
    "    '''\n",
    "    Setters for default configuration vars required in data collection process\n",
    "    '''\n",
    "    global vars_\n",
    "    vars_['APP_TOKEN'] = \"jlyZazYtAq9hSQNf0Ow4pNySj\"\n",
    "    vars_['LIMIT'] = limit\n",
    "    vars_['DATASET'] = dataset\n",
    "    vars_['BASE_URL'] = \"https://data.cityofnewyork.us/resource/\"\n",
    "    vars_['URI_IDENTIFIERS'] = ['5uac-w243', 'qgea-i56i']\n",
    "    vars_['START_DATE'] = start_date\n",
    "    vars_['END_DATE'] = end_date\n",
    "    vars_['RES_PER_RESPONSE'] = results_per_response\n",
    "    vars_['SAVE_FILE_ENDING'] = 'Historic' if vars_['DATASET'] == 'qgea-i56i' else 'YTD'\n",
    "    vars_['SAVE_TO_DIRECTORY'] = save_to_directory + '_' + vars_['SAVE_FILE_ENDING']\n",
    "    \n",
    "    \n",
    "    if all(map(lambda x: x is not None, vars_.values())):\n",
    "         print(\"Log: Defaults successfully loaded.\")\n",
    "    else:\n",
    "         print(\"Log: Some defaults weren't loaded correctly.\")\n",
    "\n",
    "def isValid(dataset, start_date, end_date, save_to_directory, results_per_response=10000, limit=100000):\n",
    "    '''\n",
    "    Performs validation checks on the input from user\n",
    "    @params:\n",
    "    data_uri := uniform resource identifier associated with the data to scrape from the API\n",
    "    data_sz := limit / dataset size to scrape\n",
    "\n",
    "    @return:\n",
    "    #need to reframe this..\n",
    "        1. [If dataset == none || data_sz  != int || data_sz != +ve int || (start_date && end_date) != datettime] := returns -1\n",
    "        2. [If data_sz == none := returns 0 and sets default results_per_responsee = 1000\n",
    "        3. [Everything works perfect := returns 1] \n",
    "    '''\n",
    "    \n",
    "    global vars_  \n",
    "    def validDate(date): #util for date validation\n",
    "         try: \n",
    "            parse(date, fuzzy=True)\n",
    "            print(\"Correct date\")\n",
    "            return True\n",
    "         except ValueError:\n",
    "            print(\"Error: Incorrect date.\")\n",
    "            return False\n",
    "\n",
    "    if dataset is None: #dataset uri is empty\n",
    "        print(\"Error: Invalid URI.\")\n",
    "        return -1\n",
    "\n",
    "    elif results_per_response is None: #limit is empty\n",
    "        print(\"Log: Results per response is empty. Setting default as 10000.\")\n",
    "        return 0\n",
    "\n",
    "    elif start_date is None: #start_date is empty\n",
    "        print(\"Error: Start date is empty.\")\n",
    "        return -1\n",
    "        \n",
    "    elif end_date is None: #start_date is empty\n",
    "        print(\"Error: End date is empty.\")\n",
    "        return -1\n",
    "         \n",
    "    elif save_to_directory is None: #savetodirectory is empty\n",
    "        print(\"Error: Directory to write data is not present.\")\n",
    "        return -1\n",
    "    \n",
    "    elif results_per_response is None: #max limit is empty\n",
    "        print(\"Log: Results per response is empty. Setting default as 10000.\")\n",
    "        return 0\n",
    "    \n",
    "    elif limit is None: #max limit is empty\n",
    "        print(\"Log: Default limit is empty. Setting default as 100000.\")\n",
    "        return 0\n",
    "         \n",
    "    elif dataset and start_date and end_date:  #params aren't empty\n",
    "        #performing validation checks on data \n",
    "        try: \n",
    "            results_per_response = int(results_per_response) #check if results_per_response is !<= 0\n",
    "            if results_per_response <= 0:\n",
    "                print(\"Error: Specifid results per response is not > 0.\")\n",
    "                return -1\n",
    "        except ValueError:\n",
    "            print(\"Error: Not an integer.\")\n",
    "            return -1\n",
    "        \n",
    "        try: \n",
    "            limit = int(limit) #check if limit is !<= 0\n",
    "            if limit <= 0:\n",
    "                print(\"Error: Specifid results per response is not > 0.\")\n",
    "                return -1\n",
    "        except ValueError:\n",
    "            print(\"Error: Not an integer.\")\n",
    "            return -1\n",
    "\n",
    "        if validDate(start_date) != True: #start_date is invalid\n",
    "            return -1\n",
    "\n",
    "        if validDate(end_date) != True: #end_date is invalid\n",
    "            return -1\n",
    "\n",
    "        parts = str(dataset).split('-') #check if dataset_uri length is perfect\n",
    "        if len(parts[0]) != 4 or len(parts[1]) != 4:\n",
    "            print(\"Error: Invalid URI.\")\n",
    "            return -1\n",
    "\n",
    "        #Some cheating here... hardcoding values for validation checks\n",
    "        if dataset not in {'5uac-w243','qgea-i56i'}: #check if its the right dataset_uri\n",
    "            print(\"Error: Invalid Dataset URI.\")\n",
    "            return -1\n",
    "        \n",
    "        p1 = Path(os.getcwd() + '\\data_Historic')\n",
    "        p2 = Path(os.getcwd() + '\\data_YTD')\n",
    "        \n",
    "        if not p1.exists():\n",
    "            print(str(p1))\n",
    "            print(\"Error: Path to save csv files for Historic data doesn't exist.\")\n",
    "            return -1\n",
    "        \n",
    "        if not p2.exists():\n",
    "            print(str(p2))\n",
    "            print(\"Error: Path to save csv files for Year To Date data doesn't exist.\")\n",
    "            return -1\n",
    "\n",
    "        \n",
    "        print(\"Log: Validations passed.\")\n",
    "        return 1\n",
    "\n",
    "def isValidAndSetDefaults(dataset_uri, start_date, end_date, save_to_directory, results_per_response=10000, limit=100000):\n",
    "    '''\n",
    "    check if everything isvalid and defaults are loaded\n",
    "    @return: boolean variable True or False\n",
    "    @params:\n",
    "    dataset_uri := uniform resource identifier associated with the data to scrape from the API\n",
    "    start_date := date to start scraping the dataset from\n",
    "    end_date := date to end scraping the dataset from\n",
    "    save_to_directory := target location where the csvs are to be stored\n",
    "    results_per_responses := number of records to scrape per API request\n",
    "    limit := max limit on number of records to scrape globally\n",
    "    '''\n",
    "    \n",
    "    ret_value = isValid(dataset_uri, start_date, end_date, save_to_directory)\n",
    "    if ret_value == -1: #something isn't working correctly. Check logs\n",
    "        print(\"Error: Unable to set defaults.\")\n",
    "        return False\n",
    "    elif ret_value == 0: #everything is working correctly, except no default params specified in the data fetch operation.\n",
    "        setDefaults(dataset_uri, start_date, end_date, save_to_directory)\n",
    "        return True     \n",
    "    elif ret_value == 1:\n",
    "        setDefaults(dataset_uri, start_date, end_date, save_to_directory, results_per_response, limit)\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Error: Unknown error.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform API Request and store the response in csv file\n",
    "\n",
    "1. `getData()`: helper function to support the below operations  \n",
    "    1.1 `buildURL()` : constructs URL from base URL and global vars_  \n",
    "    1.2 `callAPI()` :  helper function that performs data fetch operation  \n",
    "    1.3 `getDataByChunks()` : main function that builds the data by fetching it in chunks and dumping into the csv files  \n",
    "2. `mergeCSV()`: merges the resultant csv files obtained from getData() to a single resultant csv file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "    global vars_ #accessing globals\n",
    "    \n",
    "    #------------------------#Helpers for the function getData----------------------#  \n",
    "    def buildURL():\n",
    "        '''\n",
    "        Builds complete URL to be used for scraping\n",
    "        @return: returns complete url to use for scraping\n",
    "        ''' \n",
    "        url = str(vars_['BASE_URL'] + vars_['DATASET']) + \".json?$where=cmplnt_fr_dt >='{}' and cmplnt_fr_dt < '{}'&$limit={}&$order=cmplnt_fr_dt\".format(vars_['START_DATE'], vars_['END_DATE'], vars_['RES_PER_RESPONSE'])\n",
    "        print(url)\n",
    "        return url\n",
    "\n",
    "    def callAPI(url):\n",
    "        '''\n",
    "        Performs API request to get data with appropriate token and receives response\n",
    "        @params: complete URL\n",
    "        @return: If response_code == 200 := json dump of received response, else None \n",
    "        '''\n",
    "        token = vars_['APP_TOKEN'] #get the APP_TOKEN defined in the vars_\n",
    "        header = {\"X-App-Token\": token} #pass the APP_Token as the header in the request\n",
    "        res = requests.get(url, headers = header, verify = False) #perform actual get operation of request\n",
    "        print(f\"HTTP response code: {res.status_code}\") \n",
    "        if (res.status_code == 200):#check for status code\n",
    "            results = res.json() #parse the request.response to json\n",
    "            return results\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def getDataByChunks():\n",
    "        '''\n",
    "        main function that performs data fetch operation in chunks as the data is very huge\n",
    "        \n",
    "        '''\n",
    "        offset = 0\n",
    "        prevFound = vars_['RES_PER_RESPONSE'] - 1\n",
    "\n",
    "        while(prevFound > 0):\n",
    "            url = buildURL()\n",
    "            url += \"&$offset={}\".format(offset) #Appending offset to set start of records in dataset\n",
    "            \n",
    "            print(f\"Calling offset {offset} : {url}\")\n",
    "            results = callAPI(url)\n",
    "\n",
    "            if (results == None): #if the received response is invalid or has no fetched records\n",
    "                prevFound = 0\n",
    "            else:            \n",
    "                prevFound = len(results)\n",
    "                print(\"Results Found:\", prevFound)\n",
    "\n",
    "                if (prevFound > 0):\n",
    "                    df = pd.io.json.json_normalize(results) #normalise the results from json to dataframe\n",
    "                    firstDate = df[\"cmplnt_fr_dt\"].head(1) \n",
    "                    lastDate = df[\"cmplnt_fr_dt\"].tail(1)\n",
    "                    print(\"First Date:\", firstDate)\n",
    "                    print(\"Last Date:\", lastDate)\n",
    "                    fileName = \"NewYork_Crime_\" + vars_['START_DATE'] + \"_\" + vars_['END_DATE'] + \"_\" + str(offset).zfill(20) + \".csv\" #complete filename to store each datachunk\n",
    "                    print(f\"Saving {prevFound} records to fileName: {fileName}\")\n",
    "                    p = Path(os.getcwd() + '\\\\' + vars_['SAVE_TO_DIRECTORY'] + '\\\\') #Bad design.. trying to get path on the fly..must be system variable\n",
    "                    df.to_csv(os.path.join(str(p), fileName), index = False) #parse to csv each file\n",
    "                    offset = offset + vars_['RES_PER_RESPONSE']\n",
    "                    time.sleep(5) # to pause for 5 seconds before starting next round of data fetch operation for each datachunk\n",
    "                    print(\"--------------------------------------------------------------------------------------------------------\")        \n",
    "    getDataByChunks()\n",
    "    #-------------------------------------######------------------------------------#\n",
    "\n",
    "def mergeCSV():\n",
    "    '''\n",
    "    Merge multiple csv's generated by getData() from the vars_['SAVE_TO_DIRECTORY'] into single csv\n",
    "    @return: merged_csv file\n",
    "    '''\n",
    "    \n",
    "    merge_header = \"NYPD_Complaint_Data_\" #string to use in naming csv filenames while performing writes\n",
    "    print(f\"Current path - {os.getcwd()} + '\\\\' + {vars_['SAVE_TO_DIRECTORY']} + '\\\\'\")\n",
    "    path = Path(os.getcwd() + '\\\\' + vars_['SAVE_TO_DIRECTORY'] + '\\\\') #i think i need this in global\n",
    "    all_files = glob(os.path.join(path, \"*.csv\")) #find all csv files in the path\n",
    "    concatenated = []\n",
    "    total_files = len(all_files)\n",
    "    print(f\"Total files - {total_files}\") #check total files length and then loop over all the files in the csv\n",
    "    if total_files > 0: #validation check\n",
    "        print(\"Merge started.\")\n",
    "        for filename in all_files:\n",
    "            df = pd.read_csv(filename, index_col=None, header=0) #parse the file and store it to df\n",
    "            concatenated.append(df) #store all the dfs in list.\n",
    "            print(f\"Log: loaded file- {filename} in dataframe\") #logging for each filename\n",
    "        frame = pd.concat(concatenated, axis=0, sort = True) #expensive operation for larger datasets\n",
    "        \n",
    "        #set frame header to be used as csv filename\n",
    "        frame_header = merge_header + vars_['SAVE_FILE_ENDING'] + \".csv\"\n",
    "        print(frame_header) #logging for combined dataframe file in the end\n",
    "        frame.to_csv(frame_header) #parse the final concatenated df to csv \n",
    "    else:\n",
    "        print(\"Error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define entry-point helper function to call getData() on two API endpoints\n",
    "\n",
    "1. `preprocessData()`: preprocesses data and returns two df respectively for historic and yearToDate  \n",
    "    1.1 `preprocessHistoric()`:  preprocess data and returns df for historic data  \n",
    "    1.2 `preprocessYearToDate()`:  preprocess data and returns df for yearToDate data  \n",
    "\n",
    "<b>Note</b> - \n",
    "\n",
    "<b><i>To trigger this API call for 6 years, you have to manually change the dates in the `preprocessHistoric()` and `preprocessYearToDate()`</i></b>\n",
    "\n",
    "* For `preprocessHistoric`, a sample setup for API request will be as:  \n",
    "\n",
    "`def preprocessHistoric(dataURI):\n",
    "        isValidAndSetDefaults(dataURI, <start_date_as_string>, <end_date_as_string>, \"data\") #This is the line that has to be modified\n",
    "`\n",
    "<hr></hr>\n",
    "\n",
    "* For `preprocessYearToDate`, a sample setup for API request will be as:  \n",
    "\n",
    "`def preprocessYearToDate(dataURI):\n",
    "        isValidAndSetDefaults(dataURI, <start_date_as_string>, <end_date_as_string>, \"data\") #This is the line that has to be modified\n",
    "`\n",
    "\n",
    "<b><i>Also, there must be empty directories created with exact name as `data_Historic` and `data_YTD` in the same directory where this notebook exists to ensure that this code runs successfully. Without these directories, the user will not be able to send the API request</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Pre-processing NYPD_Complaint_Data Historic records---------------\n",
      "Correct date\n",
      "Correct date\n",
      "Log: Validations passed.\n",
      "Log: Defaults successfully loaded.\n",
      "Default configurations -\n",
      " {'APP_TOKEN': 'jlyZazYtAq9hSQNf0Ow4pNySj', 'LIMIT': 100000, 'DATASET': 'qgea-i56i', 'BASE_URL': 'https://data.cityofnewyork.us/resource/', 'URI_IDENTIFIERS': ['5uac-w243', 'qgea-i56i'], 'SAVE_TO_DIRECTORY': 'data_Historic', 'START_DATE': '2018-01-01', 'END_DATE': '2018-01-30', 'RES_PER_RESPONSE': 10000, 'SAVE_FILE_ENDING': 'Historic'}\n",
      "https://data.cityofnewyork.us/resource/qgea-i56i.json?$where=cmplnt_fr_dt >='2018-01-01' and cmplnt_fr_dt < '2018-01-30'&$limit=10000&$order=cmplnt_fr_dt\n",
      "Calling offset 0 : https://data.cityofnewyork.us/resource/qgea-i56i.json?$where=cmplnt_fr_dt >='2018-01-01' and cmplnt_fr_dt < '2018-01-30'&$limit=10000&$order=cmplnt_fr_dt&$offset=0\n",
      "HTTP response code: 200\n",
      "Results Found: 10000\n",
      "First Date: 0    2018-01-01T00:00:00.000\n",
      "Name: cmplnt_fr_dt, dtype: object\n",
      "Last Date: 9999    2018-01-09T00:00:00.000\n",
      "Name: cmplnt_fr_dt, dtype: object\n",
      "Saving 10000 records to fileName: NewYork_Crime_2018-01-01_2018-01-30_00000000000000000000.csv\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "https://data.cityofnewyork.us/resource/qgea-i56i.json?$where=cmplnt_fr_dt >='2018-01-01' and cmplnt_fr_dt < '2018-01-30'&$limit=10000&$order=cmplnt_fr_dt\n",
      "Calling offset 10000 : https://data.cityofnewyork.us/resource/qgea-i56i.json?$where=cmplnt_fr_dt >='2018-01-01' and cmplnt_fr_dt < '2018-01-30'&$limit=10000&$order=cmplnt_fr_dt&$offset=10000\n",
      "HTTP response code: 200\n",
      "Results Found: 10000\n",
      "First Date: 0    2018-01-09T00:00:00.000\n",
      "Name: cmplnt_fr_dt, dtype: object\n",
      "Last Date: 9999    2018-01-18T00:00:00.000\n",
      "Name: cmplnt_fr_dt, dtype: object\n",
      "Saving 10000 records to fileName: NewYork_Crime_2018-01-01_2018-01-30_00000000000000010000.csv\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "https://data.cityofnewyork.us/resource/qgea-i56i.json?$where=cmplnt_fr_dt >='2018-01-01' and cmplnt_fr_dt < '2018-01-30'&$limit=10000&$order=cmplnt_fr_dt\n",
      "Calling offset 20000 : https://data.cityofnewyork.us/resource/qgea-i56i.json?$where=cmplnt_fr_dt >='2018-01-01' and cmplnt_fr_dt < '2018-01-30'&$limit=10000&$order=cmplnt_fr_dt&$offset=20000\n",
      "HTTP response code: 200\n",
      "Results Found: 10000\n",
      "First Date: 0    2018-01-18T00:00:00.000\n",
      "Name: cmplnt_fr_dt, dtype: object\n",
      "Last Date: 9999    2018-01-25T00:00:00.000\n",
      "Name: cmplnt_fr_dt, dtype: object\n",
      "Saving 10000 records to fileName: NewYork_Crime_2018-01-01_2018-01-30_00000000000000020000.csv\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "https://data.cityofnewyork.us/resource/qgea-i56i.json?$where=cmplnt_fr_dt >='2018-01-01' and cmplnt_fr_dt < '2018-01-30'&$limit=10000&$order=cmplnt_fr_dt\n",
      "Calling offset 30000 : https://data.cityofnewyork.us/resource/qgea-i56i.json?$where=cmplnt_fr_dt >='2018-01-01' and cmplnt_fr_dt < '2018-01-30'&$limit=10000&$order=cmplnt_fr_dt&$offset=30000\n",
      "HTTP response code: 200\n",
      "Results Found: 5089\n",
      "First Date: 0    2018-01-25T00:00:00.000\n",
      "Name: cmplnt_fr_dt, dtype: object\n",
      "Last Date: 5088    2018-01-29T00:00:00.000\n",
      "Name: cmplnt_fr_dt, dtype: object\n",
      "Saving 5089 records to fileName: NewYork_Crime_2018-01-01_2018-01-30_00000000000000030000.csv\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "https://data.cityofnewyork.us/resource/qgea-i56i.json?$where=cmplnt_fr_dt >='2018-01-01' and cmplnt_fr_dt < '2018-01-30'&$limit=10000&$order=cmplnt_fr_dt\n",
      "Calling offset 40000 : https://data.cityofnewyork.us/resource/qgea-i56i.json?$where=cmplnt_fr_dt >='2018-01-01' and cmplnt_fr_dt < '2018-01-30'&$limit=10000&$order=cmplnt_fr_dt&$offset=40000\n",
      "HTTP response code: 200\n",
      "Results Found: 0\n",
      "Current path - C:\\Users\\Chirag\\Desktop\\sem2-junk\\ML-datasets\\data-science-in-python\\try_another + '\\' + data_Historic + '\\'\n",
      "Total files - 4\n",
      "Merge started.\n",
      "Log: loaded file- C:\\Users\\Chirag\\Desktop\\sem2-junk\\ML-datasets\\data-science-in-python\\try_another\\data_Historic\\NewYork_Crime_2018-01-01_2018-01-30_00000000000000000000.csv in dataframe\n",
      "Log: loaded file- C:\\Users\\Chirag\\Desktop\\sem2-junk\\ML-datasets\\data-science-in-python\\try_another\\data_Historic\\NewYork_Crime_2018-01-01_2018-01-30_00000000000000010000.csv in dataframe\n",
      "Log: loaded file- C:\\Users\\Chirag\\Desktop\\sem2-junk\\ML-datasets\\data-science-in-python\\try_another\\data_Historic\\NewYork_Crime_2018-01-01_2018-01-30_00000000000000020000.csv in dataframe\n",
      "Log: loaded file- C:\\Users\\Chirag\\Desktop\\sem2-junk\\ML-datasets\\data-science-in-python\\try_another\\data_Historic\\NewYork_Crime_2018-01-01_2018-01-30_00000000000000030000.csv in dataframe\n",
      "NYPD_Complaint_Data_Historic.csv\n",
      "------------Pre-processing NYPD_Complaint_Data Year-To-Date records---------------\n",
      "Correct date\n",
      "Correct date\n",
      "Log: Validations passed.\n",
      "Log: Defaults successfully loaded.\n",
      "Default configurations -\n",
      " {'APP_TOKEN': 'jlyZazYtAq9hSQNf0Ow4pNySj', 'LIMIT': 100000, 'DATASET': '5uac-w243', 'BASE_URL': 'https://data.cityofnewyork.us/resource/', 'URI_IDENTIFIERS': ['5uac-w243', 'qgea-i56i'], 'SAVE_TO_DIRECTORY': 'data_YTD', 'START_DATE': '2019-01-01', 'END_DATE': '2019-01-30', 'RES_PER_RESPONSE': 10000, 'SAVE_FILE_ENDING': 'YTD'}\n",
      "https://data.cityofnewyork.us/resource/5uac-w243.json?$where=cmplnt_fr_dt >='2019-01-01' and cmplnt_fr_dt < '2019-01-30'&$limit=10000&$order=cmplnt_fr_dt\n",
      "Calling offset 0 : https://data.cityofnewyork.us/resource/5uac-w243.json?$where=cmplnt_fr_dt >='2019-01-01' and cmplnt_fr_dt < '2019-01-30'&$limit=10000&$order=cmplnt_fr_dt&$offset=0\n",
      "HTTP response code: 200\n",
      "Results Found: 10000\n",
      "First Date: 0    2019-01-01T00:00:00.000\n",
      "Name: cmplnt_fr_dt, dtype: object\n",
      "Last Date: 9999    2019-01-08T00:00:00.000\n",
      "Name: cmplnt_fr_dt, dtype: object\n",
      "Saving 10000 records to fileName: NewYork_Crime_2019-01-01_2019-01-30_00000000000000000000.csv\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "https://data.cityofnewyork.us/resource/5uac-w243.json?$where=cmplnt_fr_dt >='2019-01-01' and cmplnt_fr_dt < '2019-01-30'&$limit=10000&$order=cmplnt_fr_dt\n",
      "Calling offset 10000 : https://data.cityofnewyork.us/resource/5uac-w243.json?$where=cmplnt_fr_dt >='2019-01-01' and cmplnt_fr_dt < '2019-01-30'&$limit=10000&$order=cmplnt_fr_dt&$offset=10000\n",
      "HTTP response code: 200\n",
      "Results Found: 10000\n",
      "First Date: 0    2019-01-08T00:00:00.000\n",
      "Name: cmplnt_fr_dt, dtype: object\n",
      "Last Date: 9999    2019-01-17T00:00:00.000\n",
      "Name: cmplnt_fr_dt, dtype: object\n",
      "Saving 10000 records to fileName: NewYork_Crime_2019-01-01_2019-01-30_00000000000000010000.csv\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "https://data.cityofnewyork.us/resource/5uac-w243.json?$where=cmplnt_fr_dt >='2019-01-01' and cmplnt_fr_dt < '2019-01-30'&$limit=10000&$order=cmplnt_fr_dt\n",
      "Calling offset 20000 : https://data.cityofnewyork.us/resource/5uac-w243.json?$where=cmplnt_fr_dt >='2019-01-01' and cmplnt_fr_dt < '2019-01-30'&$limit=10000&$order=cmplnt_fr_dt&$offset=20000\n",
      "HTTP response code: 200\n",
      "Results Found: 10000\n",
      "First Date: 0    2019-01-17T00:00:00.000\n",
      "Name: cmplnt_fr_dt, dtype: object\n",
      "Last Date: 9999    2019-01-26T00:00:00.000\n",
      "Name: cmplnt_fr_dt, dtype: object\n",
      "Saving 10000 records to fileName: NewYork_Crime_2019-01-01_2019-01-30_00000000000000020000.csv\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "https://data.cityofnewyork.us/resource/5uac-w243.json?$where=cmplnt_fr_dt >='2019-01-01' and cmplnt_fr_dt < '2019-01-30'&$limit=10000&$order=cmplnt_fr_dt\n",
      "Calling offset 30000 : https://data.cityofnewyork.us/resource/5uac-w243.json?$where=cmplnt_fr_dt >='2019-01-01' and cmplnt_fr_dt < '2019-01-30'&$limit=10000&$order=cmplnt_fr_dt&$offset=30000\n",
      "HTTP response code: 200\n",
      "Results Found: 3952\n",
      "First Date: 0    2019-01-26T00:00:00.000\n",
      "Name: cmplnt_fr_dt, dtype: object\n",
      "Last Date: 3951    2019-01-29T00:00:00.000\n",
      "Name: cmplnt_fr_dt, dtype: object\n",
      "Saving 3952 records to fileName: NewYork_Crime_2019-01-01_2019-01-30_00000000000000030000.csv\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "https://data.cityofnewyork.us/resource/5uac-w243.json?$where=cmplnt_fr_dt >='2019-01-01' and cmplnt_fr_dt < '2019-01-30'&$limit=10000&$order=cmplnt_fr_dt\n",
      "Calling offset 40000 : https://data.cityofnewyork.us/resource/5uac-w243.json?$where=cmplnt_fr_dt >='2019-01-01' and cmplnt_fr_dt < '2019-01-30'&$limit=10000&$order=cmplnt_fr_dt&$offset=40000\n",
      "HTTP response code: 200\n",
      "Results Found: 0\n",
      "Current path - C:\\Users\\Chirag\\Desktop\\sem2-junk\\ML-datasets\\data-science-in-python\\try_another + '\\' + data_YTD + '\\'\n",
      "Total files - 4\n",
      "Merge started.\n",
      "Log: loaded file- C:\\Users\\Chirag\\Desktop\\sem2-junk\\ML-datasets\\data-science-in-python\\try_another\\data_YTD\\NewYork_Crime_2019-01-01_2019-01-30_00000000000000000000.csv in dataframe\n",
      "Log: loaded file- C:\\Users\\Chirag\\Desktop\\sem2-junk\\ML-datasets\\data-science-in-python\\try_another\\data_YTD\\NewYork_Crime_2019-01-01_2019-01-30_00000000000000010000.csv in dataframe\n",
      "Log: loaded file- C:\\Users\\Chirag\\Desktop\\sem2-junk\\ML-datasets\\data-science-in-python\\try_another\\data_YTD\\NewYork_Crime_2019-01-01_2019-01-30_00000000000000020000.csv in dataframe\n",
      "Log: loaded file- C:\\Users\\Chirag\\Desktop\\sem2-junk\\ML-datasets\\data-science-in-python\\try_another\\data_YTD\\NewYork_Crime_2019-01-01_2019-01-30_00000000000000030000.csv in dataframe\n",
      "NYPD_Complaint_Data_YTD.csv\n"
     ]
    }
   ],
   "source": [
    "def preprocessData():\n",
    "    '''\n",
    "    Here, I manually changed the dates on every run to get data for 6 months for 6 years from 2013 - 2019\n",
    "    We can changes\n",
    "    @return: two dataframes := historic and yearToDate\n",
    "    '''\n",
    "    def preprocessHistoric(dataURI):\n",
    "        '''\n",
    "        @params:dataURI == 'qgea-i56i'\n",
    "        Here, the API base endpoint for historic data is https://data.cityofnewyork.us/Public-Safety/NYPD-Complaint-Data-Historic/qgea-i56i \n",
    "        '''\n",
    "        if dataURI is None:\n",
    "            print(\"Error: DataURI is missing.\")\n",
    "        print(\"------------Pre-processing NYPD_Complaint_Data Historic records---------------\")\n",
    "        isValidAndSetDefaults(dataURI, \"2018-01-01\", \"2018-06-30\", \"data\")\n",
    "        print(f\"Default configurations -\\n {vars_}\")\n",
    "        getData() # -- comment this code out once its done scraping data\n",
    "        mergeCSV() # -- comment this code out once a final merged csv is formed in the root folder\n",
    "        \n",
    "    def preprocessYearToDate(dataURI):\n",
    "        '''\n",
    "        @params:dataURI == '5uac-w243'\n",
    "        Here, the API base endpoint for yearToDate data is https://data.cityofnewyork.us/Public-Safety/NYPD-Complaint-Data-Historic/5uac-w243 \n",
    "        '''\n",
    "        if dataURI is None:\n",
    "            print(\"Error: DataURI is missing.\")\n",
    "        print(\"------------Pre-processing NYPD_Complaint_Data Year-To-Date records---------------\")\n",
    "        isValidAndSetDefaults(dataURI, \"2019-01-01\", \"2019-06-30\", \"data\")\n",
    "        print(f\"Default configurations -\\n {vars_}\")\n",
    "        getData() # -- comment this code out once its done scraping data\n",
    "        mergeCSV() # -- comment this code out once a final merged csv is formed in the root folder\n",
    "    \n",
    "    #comment the next two lines after pulling data from source\n",
    "    preprocessHistoric(\"qgea-i56i\")\n",
    "    preprocessYearToDate(\"5uac-w243\")   \n",
    "    \n",
    "    df_historic = pd.read_csv(\"NYPD_Complaint_Data_Historic.csv\", index_col=0) #read historic\n",
    "    df_ytd = pd.read_csv(\"NYPD_Complaint_Data_YTD.csv\", index_col=0) #read ytd \n",
    "    return df_historic, df_ytd\n",
    "    \n",
    "dfH, dfY = preprocessData()\n",
    "dfHplusY = pd.concat([dfH, dfY], sort=True) #concatenate both the historic and yearToDate df to get final df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the above cell produces a sample API Call for 2 datasets.\n",
    "1. Historic dataset between date range => (\"2018-01-01\", \"2018-01-30\")  \n",
    "2. Year to date dataset between date range => (\"2019-01-01\", \"2019-01-30\")\n",
    "\n",
    "However, further analysis is been performed on the complete dataset of 6 years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scraping data of 6 years, following are the dataframe shapes obtained:\n",
    "1. Historic data shape (dfH) = (1393340, 36)\n",
    "2. Year To Date data shape (dfY) = (218610, 36)\n",
    "3. Combined Data shape (dfHplusY) = (1611950, 36)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "9facce24-7261-40d1-accc-d6dcf73a792b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.712px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
